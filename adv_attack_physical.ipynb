{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a797f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "from adversarial_attack.fgsm import FGSM\n",
    "from adversarial_attack.pgd import PGD\n",
    "from adversarial_attack.bim import BIM\n",
    "from adversarial_attack.utils import compute_accuracy, compute_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca4fbe",
   "metadata": {},
   "source": [
    "#### Create the HT model and import pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5fbde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking...0/3\n",
      "Read 1M Feature Traces.\n",
      "Chunking...1/3\n",
      "Read 2M Feature Traces.\n",
      "Chunking...2/3\n",
      "Read 3M Feature Traces.\n",
      "Chunking...Last\n",
      "Read 4M Feature Traces.\n",
      "Iterable Dataset Loaded...\n",
      "===TEST===\n",
      "Chunking...0/1\n",
      "Chunking...Last\n",
      "Read 1M Feature Traces.\n",
      "Iterable Dataset Loaded...\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dnn_model.CNN_netlist_softma_save_resluts import Classifier_Netlist\n",
    "\n",
    "with open('save/source_config.pkl', 'rb') as pickle_file:\n",
    "    source_config_copy = pickle.load(pickle_file)\n",
    "\n",
    "path = '/home/erastus/Desktop/Postdoc_projects/Adv-TruDetect/json_temp_file/word2vec_emb/CNN_model_pretrained.pth'\n",
    "HTnn_net = Classifier_Netlist(group_id=str(2), base_path='json_temp_file', source_config=source_config_copy, pretrained=path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8d5ec2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Any\n",
    "\n",
    "def get_all_text_labels(dataloader: DataLoader) -> List[str]:\n",
    "    text_labels = []\n",
    "    for batch in dataloader:\n",
    "        text_label = batch[2]  \n",
    "        text_labels.extend(text_label)\n",
    "    return list(set(text_labels))  # Remove duplicates\n",
    "\n",
    "def get_samples_by_text_label(dataloader: DataLoader, target_text: str) -> List[Any]:\n",
    "    matching_samples = []\n",
    "    for batch in dataloader:\n",
    "        data, class_label, text_label = batch \n",
    "        for i, label in enumerate(text_label):\n",
    "            if label == target_text:\n",
    "                matching_samples.append((data[i], class_label[i]))\n",
    "    return matching_samples\n",
    "\n",
    "def get_cmp_by_emb(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None  \n",
    "\n",
    "def get_emb_by_cmp(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if key == value:\n",
    "            return val\n",
    "    return None \n",
    "\n",
    "def get_all_embeddings(approx_list_pcp):\n",
    "    embds = []\n",
    "    for i in range(len(approx_list_pcp)):\n",
    "        n_array = [get_emb_by_cmp(HTnn_net.val_data.word2vec_dict, elem) for elem in approx_list_pcp[i]]\n",
    "        embd = torch.unsqueeze(torch.from_numpy(np.array(n_array)), 0)\n",
    "        embds.append(embd)\n",
    "    return embds\n",
    "\n",
    "def approximation_error(orig_pcp_list, approx_pcp_list):\n",
    "    error_maps = {  'i':   {'i': 0, 'and':.75, 'nnd': .25, 'or': .75, 'nor':.25, 'xor':.5, 'xnr':.5}, \n",
    "                    'and': {'i':.75, 'and':0, 'nnd': 1., 'or': .5, 'nor':.5, 'xor':.75, 'xnr':.25 }, \n",
    "                    'nnd': {'and':1., 'nnd':0, 'i': .25, 'or': .5, 'nor':.5, 'xor':.25, 'xnr':.75 }, \n",
    "                    'or':  {'and':.5, 'nnd': .5, 'i': 0.75, 'or':0, 'nor':1., 'xor':.25, 'xnr':.75 }, \n",
    "                    'nor':  {'and':.5, 'nnd': .5, 'i': 0.25, 'or':1., 'nor':0, 'xor':.75, 'xnr':.25 }, \n",
    "                    'xor':  {'and':.75, 'nnd': .25, 'i': 0.5, 'or':.25, 'nor':.75, 'xor':0, 'xnr':1. }, \n",
    "                    'xnr':  {'and':.25, 'nnd': .75, 'i': 0.5, 'or':.75, 'nor':.25, 'xor':1., 'xnr':0 }   }\n",
    "    \n",
    "    final_error = 0 \n",
    "    for i in range(len(orig_pcp_list)):\n",
    "        orig_pcp, approx_pcp = orig_pcp_list[i], approx_pcp_list[i]\n",
    "        error = 0\n",
    "        for j in range(len(orig_pcp)):\n",
    "            orig, approx = orig_pcp[j], approx_pcp[j]\n",
    "            orig_op, _= separate_letters_numbers(orig.split('_')[1])\n",
    "            approx_op, _= separate_letters_numbers(approx.split('_')[1])\n",
    "            if orig_op in ['i', 'and', 'nnd', 'or', 'nor', 'xor', 'xnor']:  \n",
    "                error += error_maps[orig_op][approx_op]\n",
    "        \n",
    "        final_error += error/len(orig_pcp)\n",
    "    \n",
    "    return final_error/len(orig_pcp_list)\n",
    "\n",
    "\n",
    "def detect_score(HT_model, approx_pcp_list):\n",
    "    input_data = torch.stack(approx_pcp_list, dim=0).to(HT_model.device)\n",
    "    out = HTnn_net.model(input_data)\n",
    "    _, pred = torch.max(out, 1) \n",
    "    return pred.sum().item()/len(approx_pcp_list)\n",
    "\n",
    "def separate_letters_numbers(input_string):\n",
    "    letters = ''.join(re.findall(r'[a-zA-Z]', input_string))\n",
    "    numbers = ''.join(re.findall(r'\\d', input_string))\n",
    "    return letters, numbers\n",
    "\n",
    "def mutate_pcp_word(pcp_word, dict):\n",
    "    mutate_list = {'i':   ['nnd', 'nor', 'xor', 'xnr'],\n",
    "                   'and': ['or', 'nor', 'xnr'],\n",
    "                   'nnd': ['or', 'nor', 'xor'],\n",
    "                   'or':  ['and', 'nnd', 'xor'],\n",
    "                   'nor': ['and', 'nnd', 'xnr'],\n",
    "                   'xor': ['nnd', 'or'],\n",
    "                   'xnr': ['and', 'nor']}\n",
    "\n",
    "    splits = pcp_word.split('_')\n",
    "    op, num = separate_letters_numbers(splits[1])\n",
    "    if op in ['i', 'and', 'nnd', 'or', 'nor', 'xor', 'xnor']:\n",
    "        new_word = splits[0]+'_'+random.choice(mutate_list[op])+str(num)+'_'+splits[2]\n",
    "        if get_emb_by_cmp(dict, new_word) != None:\n",
    "            return new_word\n",
    "    return pcp_word\n",
    "\n",
    "def mutate_pcp_list(orig_list, n_changes=1, p=0.5, dict=HTnn_net.val_data.word2vec_dict):\n",
    "    orig_pcp_copy = deepcopy(orig_list)\n",
    "    for i in range(len(orig_pcp_copy)):\n",
    "        if random.random() > p:\n",
    "            pcp_word = orig_pcp_copy[i][random.randint(0, len(orig_pcp_copy[i])-1)]\n",
    "            new_pcp_word = mutate_pcp_word(pcp_word, dict)\n",
    "            \n",
    "            for j in range(len(orig_pcp_copy)): # Apply changes to all pcps that share the word\n",
    "                for k in range(len(orig_pcp_copy[j])):\n",
    "                    if orig_pcp_copy[j][k].split('_')[1] == pcp_word.split('_')[1]:\n",
    "                        orig_pcp_copy[j][k] = orig_pcp_copy[j][k].split('_')[0]+'_'+new_pcp_word.split('_')[1]+'_'+orig_pcp_copy[j][k].split('_')[2]\n",
    "\n",
    "            n_changes -= 1\n",
    "        if n_changes == 0:\n",
    "            break\n",
    "    return orig_pcp_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784abd1",
   "metadata": {},
   "source": [
    "### Pick a trojan circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "63cbe826",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_labels = get_all_text_labels(HTnn_net.val_dataloader)\n",
    "trojan_comps_labels = []\n",
    "for elem in all_text_labels:\n",
    "    if elem.startswith(\"t\"):\n",
    "        trojan_comps_labels.append(elem)\n",
    "\n",
    "trojan_comp = trojan_comps_labels[0] ## Choose one trojna circuit (automate later)\n",
    "pcp_embs = get_samples_by_text_label(HTnn_net.val_dataloader, trojan_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c183bb",
   "metadata": {},
   "source": [
    "### Get all the PCP embeddings and componenets names for the selected trojan circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e8876077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embds, all_cmps, all_labels = [], [], []\n",
    "for pcp_emb in pcp_embs:\n",
    "    p_emb, label = pcp_emb\n",
    "    full_pcp_cmp = []\n",
    "    for i in range(5):\n",
    "        name = get_cmp_by_emb(HTnn_net.val_data.word2vec_dict, list(np.float32(p_emb[i])))\n",
    "        full_pcp_cmp.append(name)\n",
    "    \n",
    "    all_labels.append(label.item())\n",
    "    all_embds.append(p_emb)\n",
    "    all_cmps.append(full_pcp_cmp)\n",
    "\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32c34e",
   "metadata": {},
   "source": [
    "### Define a genetic search algorithm to find an optimal tradeoff between approx-error and HT-detect-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "25121a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_search(HTnn_net, orig_pcp_list, population_size, generations):\n",
    "\n",
    "    population = [mutate_pcp_list(deepcopy(orig_pcp_list), n_changes=random.randint(1, 5)) for _ in range(population_size)]  \n",
    "    for _ in range(generations):\n",
    "        \n",
    "        fitness_scores, appx_errs, dect_errs = [], [], []\n",
    "        for i in range(len(population)):\n",
    "            appx_err = approximation_error(deepcopy(orig_pcp_list), deepcopy(population[i]))\n",
    "            dect_err = detect_score(HTnn_net, get_all_embeddings(deepcopy(population[i])))\n",
    "            appx_errs.append(appx_err)\n",
    "            dect_errs.append(dect_err)\n",
    "            fitness_scores.append(appx_err+dect_err)\n",
    "        \n",
    "        print(appx_errs, dect_errs)\n",
    "        parents = random.choices(population, weights=fitness_scores, k=population_size)   \n",
    "        new_population = []\n",
    "        for i in range(0, population_size, 2):\n",
    "            parent1, parent2 = parents[i], parents[i+1]    \n",
    "            child1 = mutate_pcp_list(deepcopy(parent1))\n",
    "            child2 = mutate_pcp_list(deepcopy(parent2))\n",
    "            new_population.extend([child1, child2])\n",
    "        \n",
    "        population = new_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea5c0c",
   "metadata": {},
   "source": [
    "### Run the genetic search on the trojan circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "daea9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1375, 0.0, 0.175, 0.0, 0.1, 0.0, 0.0, 0.0] [0.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 1.0]\n",
      "[0.175, 0.1, 0.0, 0.1, 0.1, 0.1875, 0.0, 0.0] [0.0, 0.75, 1.0, 0.75, 0.0, 0.0, 1.0, 1.0]\n",
      "[0.1, 0.0, 0.05, 0.15, 0.0, 0.05, 0.1, 0.0] [0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0]\n",
      "[0.15, 0.32499999999999996, 0.0, 0.2, 0.15, 0.05, 0.1, 0.0] [1.0, 0.0, 1.0, 0.75, 0.0, 1.0, 0.0, 1.0]\n",
      "[0.1, 0.15, 0.15, 0.15, 0.05, 0.0, 0.32499999999999996, 0.2375] [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "[0.1, 0.05, 0.15, 0.175, 0.175, 0.15, 0.1, 0.15] [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.75, 0.0]\n",
      "[0.2, 0.32499999999999996, 0.225, 0.1, 0.15, 0.275, 0.2375, 0.05] [0.75, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]\n",
      "[0.275, 0.32499999999999996, 0.1875, 0.05, 0.1375, 0.2, 0.225, 0.275] [0.0, 0.0, 0.0, 1.0, 0.0, 0.75, 0.0, 0.0]\n",
      "[0.1375, 0.2, 0.5, 0.2, 0.225, 0.05, 0.375, 0.05] [0.0, 0.75, 0.0, 0.75, 0.0, 1.0, 0.0, 1.0]\n",
      "[0.05, 0.5, 0.3, 0.05, 0.525, 0.05, 0.475, 0.375] [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "genetic_search(HTnn_net, deepcopy(all_cmps), population_size=8, generations=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
